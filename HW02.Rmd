---
title: "HW02"
author: "Jackson Dial"
date: "2/1/2022"
output: word_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#Load libraries
library(tidyverse)
library(olsrr)
```

# Question 1

```{r}
dat1 <- read.csv("../HW01Data/saltBP.csv")
lm1 <- lm(BP ~ saltLevel, data = dat1)
```

### Part A

```{r}
sum1 <- summary(lm1)
bh1 <- round(sum1$coefficients[2,1],3)
bp1_2 <- mean((dat1 %>% filter(saltLevel == 1))$BP) - mean((dat1 %>% filter(saltLevel == 0))$BP)
```

Here it is shown that $\hat{\beta}_1 = \overline{BP}_1 - \overline{BP}_2$ as `r bh1` = `r round(bp1_2,3)`.

### Part B

```{r}
bh0 <- round(sum1$coefficients[1], 3)
bp2 <- mean((dat1 %>% filter(saltLevel == 0))$BP)
```

Here it is shown that $\hat{\beta}_0 = \overline{BP}_2$ as `r bh0` = `r bp2`

### Part C

__Not finished__


```{r}


```


### Part D

__Not finished__

```{r}
#bp1_2 / (s_p * sqrt())

```


# Question 2

```{r}
wt_med <- median(mtcars$wt)
mtcars2 <- mtcars %>% 
  mutate(Xi = case_when(
    wt > wt_med ~ 1,
    TRUE ~ 0
  ))
#Just to verify it worked correctly
nrow(mtcars2)
sum(mtcars2$Xi)
```

### Part A

```{r}
lm2 <- lm(mpg ~ Xi, data = mtcars2)
sum2 <- summary(lm2)
bh1_2 <- sum2$coefficients[2]

# ggplot(mtcars2, aes(x = as.factor(Xi), y = mpg))+
#   geom_point()
```

The interpretation of $\hat{\beta}_1$ is for a 1 unit increase in Xi (Xi here is binary so it is equivalent to saying changing from 0 to 1, or being below the median to above the median) the average mpg of cars in this data set decreases by `r bh1_2`.

### Part B

__Not finished__

```{r}
x <- model.matrix(lm2)
x
#
dim(x)
# get the response vector, y
y=mtcars2[,1]
y
# get matrix x transpose x
xTx <- t(x) %*% x
```

Below is the design matrix:

```{r}
xTx
```

### Part C

```{r}
lm2_c <- lm(mpg ~ wt, data = mtcars2)
sum2_c <- summary(lm2_c)
sum2_c
```

The results differ greatly in terms of $R^2$ values, both multiple and adjusted. This makes sense because the difference between the two is with the first one we are using a binary variable which was made from the continuous variable that was used in the second model. Whenever we take a continuous variable and make it binary, we lose information about our data, and particularly about our response variable. The second model, that with the continuous variable of weight, is a better model.

### Part D

```{r}
lm2_d <- lm(mpg ~ Xi * hp, data = mtcars2)
sum2_d <- summary(lm2_d)
sum2_d
```

The estimated model for this regression is as follows:

$$
\hat{Y_i} =\hat{\beta_0} + \hat{\beta_1}*X_i +\hat{\beta_2}*HP_i + \hat{\beta_3}*X_i*HP_i
$$
```{r}
bh0_2d <- sum2_d$coefficients[1]
bh1_2d <- sum2_d$coefficients[2]
bh2_2d <- sum2_d$coefficients[3]
bh3_2d <- sum2_d$coefficients[4]

ggplot(mtcars2, aes(x = hp, y = mpg))+
  geom_point(aes(col = as.factor(Xi)))
```

To interpret the interaction term, which includes a binary variable ($X_i$), we will show the output of the model with each level of the variable.

First, with $X_i = 1$ or in other words, where weight is greater than the median.

$$
\hat{Y_i} = 31.82 + (-11.48*1) +(-0.0697*HP_i) + (0.0449*1*HP_i)
$$

And now with $X_i = 0$ or where weight is equal to or less than the median.

$$
\hat{Y_i} = 31.82 + (-11.48*0) +(-0.0697*HP_i) + (0.0449*0*HP_i)
$$

We can see here that the interaction term, which is the last term in each of the above expressions, changes the effect of HP on the model. In the first example where $X_i = 1$, we have an increased effect on the predicted mpg value coming from HP. When we set $X_i = 0$ in the second model, that 'extra' effect of HP on mpg is removed. Thus it is shown that the interaction changes how one predictor variable changes the response variable.

This model is _NOT_ additive.

### Part E

```{r}
lm2_e <- lm(mpg ~ wt*hp, data = mtcars2)
sum2_e <- summary(lm2_e)
sum2_e
sum2_e_bh3p <- sum2_e$coefficients[4,4]
```


To interpret the interaction term, we will manipulate our mathematical model which is given below:


$$
\hat{Y_i} =\hat{\beta_0} + \hat{\beta_1}*WT_i +\hat{\beta_2}*HP_i + \hat{\beta_3}*WT_i*HP_i
$$

It can be rearranged algebraically as follows:


$$
\hat{Y_i} =\hat{\beta_0} + (\hat{\beta_2}*HP_i) + [WT_i*(\hat{\beta_1} + \hat{\beta_3}*HP_i)]
$$

We can see here now that a 1 unit increase in $WT$ will result in an increase of the predicted $Y_i$ or $MPG$ by a factor of $\hat{\beta_1}$, and $\hat{\beta_3} * HP_i$, or by an increase of $-0.1201 + (0.0279*HP_i)$.

Because our interaction term is significant, $ p \approx $ `r round(sum2_e_bh3p,5)`, we cannot interpret the main effects in the model by themselves.

# Question 3

```{r}
dat3 <- read.table("../HW02Data/HW2-prob3.dat", header = TRUE)
```

### Part A

```{r}
lm3 <- lm(y ~ ., data = dat3)
sum3 <- summary(lm3)
sum3
```

The estimator $\hat{\beta_2}$ can be interpreted to be a 1 unit increase in the $X_2$ variable is equivalent to a 8.5013 increase in y.

### Part B

```{r}
cor(dat3)
```

According to the correlation matrix, $X_4$ explains the most variability of in y.

### Part C

```{r}
lms_3c <- ols_step_all_possible(lm3)
max(lms_3c$adjr)
min(lms_3c$cp)

lmfits_3c <- ols_step_best_subset(lm3)
lmfits_3c
max(lmfits_3c$adjr)
min(lmfits_3c$cp)
min(lmfits_3c$aic)
min(lmfits_3c$sbic)
```


For $R^2_a$, the best model is #4, which includes X1, X2, X3, and X5

For $C_P$, the best model is #3, which includes X1, X2, and X3.

For $AIC_p$, the best model is #3, which includes X1, X2, and X3.

For $BIC_p$, the best model is #3, which includes X1, X2, and X3.

### Part D

The four recommendations above do not all recommend the same subset of variables. This does not always happen, however, as there could just be one model that is much better than all the other possible models.

### Part E

```{r}
ols_step_backward_p(lm3,prem = 0.1)

ols_step_forward_p(lm3,penter = 0.15)

ols_step_both_p(lm3,pent = 0.15,prem = 0.1)
```

Using backward selection, our selected model subset is X1, X2, and X3. 

Using forward selection, our selected model subset is X1, X2, X3, and X4.

Using step-wise selection, our selected model subset is is X1, X2, and X3.

